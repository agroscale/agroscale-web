```{r setup, include=FALSE}
library(knitr)
library(dismo)
library(randomForest)
library(rpart)
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical modeling 

*Robert Hijmans*, University of California, Davis


## Introduction

Statistical methods can assist in the use of evidence for description, understanding and prediction of phenomena of interest. This chapter is about *prediction*, but we start with a very brief discussion of methods for formal understanding (*inference*) as that is what most agromists are familiar with. 


## Chapter requirements.

For this chapter you need the following *R* packages: `randomForest`, `rpart`, and `dismo`. They are all available for download from CRAN [see these instructions](/installation.html) about installing *R* packages.


## Inference

Introductory statistics classes in agromonmy (and most other fields) have traditionally focused on methods to support *understanding*. These courses teach probability distribution based inference through hypothesis testing, preferably using data from randomized controlled trials (RCTs). They may also introduce linear regression. There is a good reason for that: this inferential approach underpins most researc in the sciences. Perhaps the most prominent statistician of all time, R.A. Fisher, developed the Analysis of Variance (ANOVA) method to analyze crop trials when he was employed at the Rothamsted Experimental Station. 

Data from trials (experiments) tends to have some unexplained ("random") variation because of differences in the objects used for testing. Each experimental plot is a little different, or, in the case of medicine or social sciences, each individual may respond a little different to a pill or other stimulus. In classical statistics, a main concern is, therefore, whether an observed difference between treatments is likely to be caused by chance (not a significant difference) or not (a significant difference). For example, you may employ a *t-test* to see if the effect of *treatment A* is significantly different from the effect of *treatment B*. Statistical significance is expressed as a probability (*p-value*) that an observed difference was caused by chance. The lower this probability, the more certain we are that the observed difference is not a fluke. To use this type of framework properly, great care needs to be taken in data collection and processing, to assure that the data conform to assumptions on which the tests are based. For examples, a variable may need to be normally distributed, observations may need to be independent of each other, and measurements should not have much error. 

There has been a lot of recent work on alternative inferential approaches through the development and use of model selection methods, multi-level models, and in Bayesian inference. The [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) book is a good practical resource to learn about these new approaches to scientific understanding. 

But the topic if this chapter is *prediction* not *formal inference*. Even though we won't mind to also gain understanding from our data, our goal is learning, not understanding. An analogy is that you do not necessarily need to understand how something works (for example a car) to be able to use it; but you do need to able to evaluate whether it is fit for a particular purpose, or whether you should consider getting a more useful vehicle --- and this *may* require some understanding of how cars work.   

 

## Prediction

Statistical prediction has gained a lot of attention over the past decades. In part because of new needs and the availability of much more data, and of new algorithms. Whereas inferential statistics has emphasized the need for a few (orthogonal, independent) variables and high quality data, these new algorithms are used in the context of many variables and large noisy data sets. 

Some of these new algorithms are referred to as "machine learning" methods. We avoid that term here and we use the more general term "regression" or "supervised statistical learning" methods here. This encompasses a suite of methods that start with simple linear regression and end with complex algorithms such as neural networks. These more complex methods form the backbone of much modern "data science" methods. Data science is the use of statistical methods together with efficient and reproducible procedures to acquire, curate, and manipulate (large quantities of) data. In other words, data science is at the intersection data management and software development and (supervised) statistical learning. This chapter outlines some important features of statistical learning. We focus on the model over-fitting versus under-fitting; model evaluation and interpretation; and model application. 

Much of what we discuss is based on [An Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/). This is very accessible book that you can [download for free](https://web.stanford.edu/~hastie/pub.htm#books). It comes with exercises in *R* and a lot of on-line learning materials such as [this free MOOC](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about). If the subject matter of this chapter intrigues you, you should not hesitate to read it. 


## Supervised learning

"Supervised learning" refers to building a model with a data set that includes records that have values for a response variable of interest (for example, crop yield), and for predictor variables (for example, rainfall and fertilizer application rates). 

In matrix notation these models have the form  $\hat{Y} = \hat{f}(X) + \epsilon$,  where $\hat{Y}$ (Y-hat) is a vector of model predictions for the variable of interest; $X$ is the matrix with predictor variables; and $\epsilon$ (epsilon) is the unexplained variation. The art is to find the most appropriate function $\hat{f}$ (f-hat) --- that is the one that most closely resembles the *true* function $f$. In the next paragraph, we illustrate that the best model is not necessarily the model that minimzes the unexplained variation ($\epsilon$).


## Over-fitting versus under-fitting

### linear regression model

Over-fitting a model means that the model fits the data that was used to build the model very well --- in fact too good. Let's look at that with a simple data set where we have a response variable `y` and a single predictor variable `x`. 

```{r}
x <- c(35, 47.2, 20.7, 27.1, 42.5, 40.1, 28.2, 35.8, 47.4, 
		21.3, 39, 30.6, 18.7, 8.4, 45.8, 14, 47.3, 31.5, 38, 32)
y <- c(9, 11.5, 7.6, 8.8, 9.8, 11, 8.2, 8.6, 11.4, 7.4, 
		11.1, 7.9, 6, 4.6, 12.3, 6.1, 11.4, 8.8, 10.3, 9.4)
plot(x, y)
```

We could build a simple linear regression model like this
```{r}
mlr <- lm(y~x)
plot(x, y, pch=20, cex=1.5, las=1)
abline(mlr, col="red", lwd=3) 
summary(mlr)
```

A linear regression model takes the form $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...$  where $\beta_0$ is the intercept and the other $\beta$s are slopes. In this case we found $\hat{f}$  to be  *`r cf = round(coefficients(mlr), 2); paste0("y = ", cf[1], " + ", cf[2], "x")`*.  

I hope you will agree that this looks very good. The line seems to capture the general trend; the model R^2^-0.92, and the *p-values* for the model parameters are very small (strong support for the intercept and slope being unequal to zero). But can we do better?

### Spline model

Linear regression models with a single predictor variable are not very flexible. Let's try another extreme, a spline function. A spline model is referred as a non-parametric model (this is a misnomer, it has, in fact, a lot of parameters, so many that we do not want to consider them, as we cannot easily interpret them). 

```{r}
# create the function from the data
sf <- splinefun(x, y)
# x prediction points
px <- seq(1, 50, 0.25)
# predictions
py <- sf(px)

plot(x, y, pch=20, cex=2, las=1)
abline(mlr, col="red", lwd=3) 
lines(px, py, col="blue", lwd=3)
```

Wow. Model `sf` seems perfect. Let's compare the two models with the Root Mean Squared Error (RMSE) statistic. 


### Root Mean Squared Error 

The Root Mean Squared Error (RMSE) is a commonly used metric to compare models with a quantiative repsonse. For qualitative responses measures such as kappa are used, and if the response if true/false (presence/absence) ROC-AUC is commonly used.

Here is a function that implements RMSE. 

```{r}
rmse <- function(obs, prd, na.rm=FALSE) {
	sqrt(mean((obs - prd)^2, na.rm=na.rm))
}
```
Let's test this new function. We have observed values of 1 to 10, and a three different predictions.

```{r}
# no difference, perfect model
rmse(1:10, 1:10)
# a small difference, not bad
rmse(1:10, 2:11)
# rather different, much worse than the above
rmse(1:10, 10:1)
``` 

So the more different the observed and predicted data are, the higher the RMSE. The scale of the RMSE is in units of the data. To make it easier to interpret RMSE values, you can express them relative to a Null model. A Null model is a simple, perhaps naive model, that can serve as an important benchmark. To evaluate a complex model it is often not of prime interest to know how good it does, but rather how much better it does than a Null model. 

With the example data above, a good Null Model might be the mean value of the observed data. 

```{r}
null <- rmse(1:10, mean(1:10))
null
```

A function that computes the RMSE relative to a NULL model could look like this.
```{r}
rmsenull <- function(obs, pred, na.rm=FALSE) {
	r <- rmse(obs, pred, na.rm=na.rm)
	null <- rmse(obs, mean(obs))
	(null - r) / null
}
```

Let's use it.
```{r}
rmsenull(1:10, 1:10)
rmsenull(1:10, 2:11)
rmsenull(1:10, 10:1)
```

A perfect model explains 100% of the RMSE of the Null model. In the example above, the prediction `2:11` explains `r round(100* rmsenull(1:10, 2:11))`% of the Null model. That is very good! The prediction `10:1` is *worse* than the Null model.


### Compare models

Now we can use our `rmsenull` method with our two models, and compare their performance.

For the linear regression model
```{r}
plr <- predict(mlr, data.frame(x=x))
rmsenull(y, plr)
```
And for the spline model 
```{r}
psf <- sf(x)
rmsenull(y, psf)
```

By this measure, the spline model is perfect. The linear regression model might still be fit for purpose, but it does not look as good as the spline model.

However, we made a major mistake in our model evaluation procedure. We should not evaluate the model with the same data that we used to build the model. Because if we do so, we are guarenteed to select models that are overfit. To illustrate this, let's split the data in a two samples (this is not generally the best practise, see the section on cross-validation).

```{r}
n <- length(x)
set.seed(321)
i <- sample(n, 0.5 * n)
i
xa <- x[i]
ya <- y[i]
xb <- x[-i]
yb <- y[-i]
```

Now train (fit) and test (evaluate) the models again. First use sample "a" to train and sample "b" to test. 

```{r}
mlr_a <- lm(ya~xa)
sf_a <- splinefun(xa, ya)
plr_a <- predict(mlr_a, data.frame(xa=xb))
psf_a <- sf_a(xb)
rmsenull(yb, plr_a)
rmsenull(yb, psf_a)
```
Now use sample "a" to test and sample "b" to train. 

```{r}
mlr_b <- lm(yb~xb)
sf_b <- splinefun(xb, yb)
plr_b <- predict(mlr_b, data.frame(xb=xa))
psf_b <- sf_b(xa)
rmsenull(ya, psf_b)
rmsenull(ya, plr_b)
```

```{r, echo=FALSE, include=FALSE}
e <- c(rmsenull(yb, plr_a), rmsenull(ya, plr_b), rmsenull(yb, psf_a), rmsenull(ya, psf_b), NA, NA)
e[5] <- mean(e[1:2])
e[6] <- mean(e[3:4])
e <- round(e,2)
```

In both cases the linear regression model peforms much better. The mean RMSE, relative to the Null model, for the linear regression model is `r paste("(", e[1], "+", e[2], ")/2", "=", e[5])`, much smaller than for the the spline model `r paste("(", e[3], "+", e[4], ")/2", "=", e[6])`. Also note that the RMSE for the linear regression model is similar to the RMSE computed with the model training data when it was fit will all data (`r round(rmsenull(y, plr),2)`). That is also easy to see visually. 

```{r, fig.width=10, fig.height=10}
#set up the plot
plot(x, y, las=1, cex=0, xlim=c(0,80), ylim=c(3,15))

# original models
abline(mlr, col="red", lwd=2, lty=3) 
lines(px, py, col="blue", lwd=2, lty=3)

# sample A models
abline(mlr_a, lwd=2, col="red")
lines(px, sf_a(px), lwd=2, col="blue")

# sample B models
abline(mlr_b, lwd=2, col="red", lty=2)
lines(px, sf_b(px), lwd=2, col="blue", lty=2)

# sample A and B points
points(xa, ya, cex=1.5)
points(xb, yb, cex=1.5, pch=19)

# a complex legend
legend("bottomright", pch=c(1,19,rep(NA, 10)), lty=c(NA, NA, rep(c(NA, NA,1,2,3),2)), lwd=2, , pt.cex=1.5, bg="white",
	legend=c("Sample A", "Sample B", "", "Linear regression", "sample a", "sample b", "all data", "", "Spline model", "sample a", "sample b", "all data"), 
	col=c("black", "black", "white", "white", rep("red", 3), "white", "white", rep("blue", 3))
)

```

The spline model fit the training data perfectly, but that made it predict spectacularly bad for some of testing data. The linear regression model is not very flexible, so it did not change much when it only got the subset of data. This is an important feature. The linear model has **low variance**. That is, the estimated model does not change much when it is estimated with slightly different input data (do not confuse this use of the word "variance" with how it can be used to describe variability in a sample of quantitative observations). This is very important as a model only has value if it has some generality, not if it just fits a particular dataset. Note that the RMSE are essteniall for an interpolation problem. When `x` is between 15 and 40, the spline model is much worse than the linear regression model, but the three models (all data, sample A, and sample B) are somewhat similar. However, if we were to use these models for extrapolation (in practical terms, think predicting to a different region, group of people, time period), the results would be dramatically unreliable as the different spline models show extreme, but opposite responses. This does not mean that the linear model is right --- we have no data for x < 8 or x > 48.  


### The true model 

Well, in this case, we actually do have . The data for `x` and `y` were sampled (and then rounded to 1 decimal) from a *known function* --- we do not have these for the real world. But we can use functions like that to generate data to learn about modeling methods. We used the function `f` below to generate the data used in this section.

```{r}
f <- function(x) x/10 + sin(x) + sqrt(x)
X <- seq(1,50,0.1)
plot(X, f(X), type="l")
```

And the following procedure to create the sample data.
```{r}
set.seed(2)
sx <- sample(X, 20)
sy <- f(sx)
```

So, the linear model clearly was wrong. But, then, all model are wrong. Some models are more useful than others, and the linear model might be close enough to be fit for purpose.


### Can flexible models be good?

So is a single variable linear regression always better than a spline model? No. It depends. It depends on how linear the relationship between x and y, and it also depends on the sample size. The more complex the model, and the larger the sample size, the better the spline might do. Here is simple example where the spline model does better.  

The true model.
```{r}
f <- function(x) {x - x^2 + x^3*sin(x)}
X <- seq(1,50,0.1)
Y <- f(X) 
plot(X, Y, type="l")
```

Two samples from the true model. Here we do not add error to the data --- but that is often done to simulate the effect of measurment error. It is easy to do with a function like `runif` or `rnorm`.

```{r} 
set.seed(2)
xa <- sample(X, 40)
ya <- f(xa) 
xb <- sample(X, 40)
yb <- f(xb) 
```

And compare the two models again

```{r}
mlr <- lm(ya~xa)
sf <- splinefun(xa, ya)
rmsenull(yb, predict(mlr, data.frame(xa=xb)))
rmsenull(yb, sf(xb))

mlr <- lm(yb~xb)
sf <- splinefun(xb, yb)
rmsenull(ya, predict(mlr, data.frame(xb=xa)))
rmsenull(ya, sf(xa))
```

In both instances, the spline model RSME is about five times smaller. 

```{r}
px <- seq(1, 50, 0.25)
py <- sf(px)
plot(X, Y, type="l", lwd=3, las=1)
points(xa, ya, cex=1.5)
points(xb, yb, pch=19, cex=1.5)
abline(mlr, col="red", lwd=2) 
lines(px, py, col="blue", lwd=2)

legend("bottomleft", pch=c(1,19,rep(NA, 10)), lty=c(NA, NA, rep(c(NA, NA,1,2,3),2)), lwd=2, , pt.cex=1.5, bg="white",
	legend=c("Sample A", "Sample B", "", "Linear regression", "sample a", "sample b", "all data", "", "Spline model", "sample a", "sample b", "all data"), 
	col=c("black", "black", "white", "white", rep("red", 3), "white", "white", rep("blue", 3))
)
```

### What have we learned?

We should note that these stylized examples have little to do with real data, where you might have a lot of noise and multiple observations for `y` for each `x`. Perhaps you can look at that in a simulation of your own? But we hope that this section helped illustrate (a) the risk of overfitting; (b) that models should be evaluated with data that was not used to train the model.

There is much more to say about linear regression. In practice you often have many predictor variables. Or you can create new pridictor variables by creating squares and cubes, or interactions. For example, you can do 

```mlr <- lm(ya ~ xa+I(xa^2)+I(xa^3))```

With each parameter you add flexibility to the model. That can lead to overfitting, and a loss of interpretability. See the sections on stepwise model selection, and on lasso and ridge regression in [ISLR](https://web.stanford.edu/~hastie/pub.htm#books) for detailed discussion.

Like in lasso and ridge regression, spline models can be constrained (made less flexible) through some form of regularization. In these "smoothing spline" models, there is a penalty function that reduces the amount of flexibility. 


## Complex models

There are various algorithms than can be used to make relatively complex models for predictions with large data sets, with many variables, of which several may contribute to the model, perhaps with complex interactions, and likely noisy data. Popular examples of such ("machine learning") algorithms include Random Forest, Support Vector Machines (SVM), and Artificial Neural Networks (ANN). Here we focus on the Random Forest algorithm as an example. 

To illustrate the use of Random Forest, we use some data from West Africa on soil fertility and crop response to fertilizer. These data are part of a larger study described in a forthcoming paper (Bonilla et al.). 

```{r}
d <- reagro::reagro_data("soilfert")
dim(d)
head(d)
```

Again, we create two sub-datasets.

```{r}
set.seed(2019)
i <- sample(nrow(d), 0.5*nrow(d))
d1 <- d[i,]
d2 <- d[-i,]
```

These are the variables we have.

```{r, echo=FALSE}
dtab <- data.frame(variable=c('temp', 'precip', 'ExchP', 'TotK', 'ExchAl', 'TotN', 'sand', 'clay', 'SOC', 'pH', 'AWC', 'fert'),
description=c('Average temperature', 'Annual precipitation', 'Soil exchangeble P', 'Soil total K', 'Soil exchangeble Al', 'Soil total N', 'Soil franction sand (%)', 'Soil fraction clay (%)', 'Soil organic carbon (g/kg)', 'Soil pH', 'Soil water holding capacity', 'fertilizer (index) kg/ha'))
kable(dtab)
```


## Classification and Regression Trees

Before we look at the RandomForest, we first consider what the *forest* is made up of: *trees*. Specifically, the Classification and Regression Trees (CART) algorithm. 

Suppose we were interested in estimating soil organic carbon (SOC) across locations in West Africa. Suppose we have a lot of data on soil pH and the fraction of the soil that is sand or clay (cheap and easy to measure), and precipitation and temperature data as well (which is available for any location). Can we build a model that predicts SOC from pH, sand and precipitation? 

```{r, fig.width=10}
par(mfrow=c(2,3), mai=rep(0.5, 4))
for (v in c("SOC", "pH", "precip", "temp", "clay", "sand")) {
	boxplot(d[,v], main=v)
}
```

Let's first run a linear regression model
```{r}
model <- SOC~pH+precip+temp+sand+clay
lrm <- lm(model, data=d1)
summary(lrm)
rmsenull(d2$SOC, predict(lrm, d2))
```

We see that all the predictor variables are highly significant, and the R^2^ is not that bad. Perhaps we could improve the model by including interaction terms, but let's not go down that path here. 

Instead we use CART. CART recursively partitions the data set using a threshold for one variable at a time to create groups that are as homogeneous as possible.

```{r}
library(rpart)
cart <- rpart(model, data=d1)
rmsenull(d2$SOC, predict(cart, d2))
```

We see that the rmsenull is much better than the simple (perhaps overlay simplistic) linear model. CART explains about 58% of the unexplained variation, whereas the linear model explains only 21%. That is interesting. Let's look at the model. We can inspect it like this.


```{r, fig.width=10, fig.width=8}
plot(cart)
text(cart)
``` 

It may take a little effort at first, but once you get used to it, CART models are easy to understand. You can navigate down the tree. If the condition is *true* you go left, else you go right. So if `clay < 25.75` and `precip < 1248`, SOC is predicted to be 8.7 (the value of the "leaf". That is rather low (0.9% organic C, or about 1.5% soil organic matter) as might be expected on very sandy soil under relatively dry conditions. Notice how variables are used many times, in effect creating step-functions and interactions. Also note that with this tree, we end up with 13 possible predictions (the tree has 13 leaves).  

We get a bit more detail, at the expense of visual pleasure, when we print the model like this.
```{r}
cart
``` 
This shows, for example, that if we only used the first split (clay< 25.75), we get two groups. One group with 651 observations and a predicted (i.e., average) SOC of 11.8. The other group has 191 observations, and a predicted SOC of 21.2. 

The big numbers express the remaining deviance (a goodness-of-fit statistic for a model). The Null deviance is 40810.6, but after the first split is has gone down to (15525 + 12315) = 27840.
 
The Null deviance can be computed like this 
```{r}
nulldev <- function(x) {
	sum((x - mean(x))^2)
}	
nulldev(d1$SOC)	
``` 
We can compare that with the deviance of the cart model.

```{r}
cdev <- sum(cart$frame[cart$frame$var == "<leaf>", "dev"])
cdev
rdev <- round(100 * cdev / nulldev(d1$SOC))	
rdev
```

The model has reduced the deviance to `r rdev`%.

Let's turn the data sets, and build a CART model with sub-dataset `d2` (and evaluate with `d1`).

```{r}
cart2 <- rpart(model, data=d2)
rmsenull(d1$SOC, predict(cart2, d1))
```

That is very similar to the result for `d1` (`r round(rmsenull(d2$SOC, predict(cart, d2)), 2)`).
We can also check if the CART model overfits the data much by comparing RMSE computed with the test data with the RMSE computed with the train data. 

```{r}
# model 1, test data
rmsenull(d2$SOC, predict(cart, d2))
# model 1, train data
rmsenull(d1$SOC, predict(cart, d1))
# model 2, test data
rmsenull(d1$SOC, predict(cart2, d1))
# model 2, train data
rmsenull(d2$SOC, predict(cart2, d2))
```

The rmse with the training data is higher than with the testing data. That suggest that there is some overfitting, but it is not much. In this case we also seem to have low variance, in the sense that the models look similar (see below). That is not a general result --- CART models tend to have high variance. They can also overfit the data. A lot depends on how far you let the tree grow. In this case we used default stopping rules (nodes are not split if it has fewer than 20 observations or if any of the resulting nodes would get less than 7 observations), see `?rpart.control`, that avoided these problems. Trees that are grown very deep tend to overfit: they have low bias, but very high variance. 

```{r, fig.width=10}
par(mfrow=c(1,2))
plot(cart)
text(cart, cex=0.8)

plot(cart2)
text(cart2, cex=0.8)
```

CART models can be relatively easily inspected, and can "learn" about complex interactions. Another great feature is that they are not affected by the scale or transformation of the predictor variables. But they are prone to overfitting and instability. They are seldom accurate (Hastie et al.). 


## Random Forest 


### What is a Random Forest?

Random Forest builds many (> 100) CART models (we will call them *trees*) to create a new model that tends to have low variance, predict well, but does not overfit the data. 

It would not be of any use to build the *same* tree 100s of times. Each tree is build with a bootstrapped sample of the records. A bootstrap sample is a random sample with replacement. Thus the number of records is the same for each tree, but some records are not included, and some records are included more than once. On average, about 2/3 of the records are included in a sample. Here is the computational proof of that:

```{r}
mean(replicate(10000, length(unique(sample(100, replace=TRUE)))))
```

When a Random Forest model makes a prediction, a prediction is made for each tree, and the results are aggregated (averaged). This procedure of bootstrapping and aggregate, is called "bootstrap-aggregation" or "bagging".

Let's illustrate bagging by making 10 models with boostrapped data. Each model is used to make a prediction to test data, and evaluated.  

```{r}
n <- 10
set.seed(99)
predictions <- matrix(nrow=nrow(d2), ncol=n)
eval <- rep(NA, n)
for (i in 1:n) {
	k <- sample(nrow(d1), replace=TRUE)
	cartmod <- rpart(model, data=d1[k,])
	p <- predict(cartmod, d2)
	eval[i] <- rmsenull(d2$SOC, p)
	predictions[, i] <- p
}
```

For each "unseen" case in `d2` we now have five predictions 
```{r}
head(predictions)
```

We can average the individual model predictions to get our ensemble prediction.

```{r}
pavg <- apply(predictions, 1, mean)
```

The quality of the individual models
```{r}
round(eval, 3)
```

But the score for the ensemble model is quite a bit higher than the mean value is for the individual models!
```{r}
mean(eval)
rmsenull(d2$SOC, pavg)
```

And also higher than the original model (without bootstrapping)....

```{r}
rmsenull(d2$SOC, predict(cart, d2))
```

In Rob Shapire's famous words: *"many weak learners can make a strong learner"*. (It seems that this is not only true for statistical models, but that it also holds for humans).
	
Random Forest has another randomization procedure. In a regression tree, the data is partitioned at each node using the best variable, that is, the variable that can most reduce the variance in the data. In Random Forest, only a random subset of all variables (for example one third) is available at each split (node). Although this further weakens the trees, it also makes them less correlated, which is a good feature (there is not much to gain from having many very similar trees). 
	

###

Enough said. Let's create a Random Forest model.

```{r}
library(randomForest)
rf <- randomForest(model, data=d1)
rf
```	

That's it. Given reasonable software for data analysis, creating a machine learning model is just as complicated as creating a linear regression model. In this type of work the effort is to compile the data, to select a method, and to evaluate the results.

Compare reported results with ours 

```{r}
p <- predict(rf, d2)
rmsenull(d2$SOC, p)
```

```{r}
# Mean of squared residuals
rmse(d2$SOC, p)^2
# % Var explained:
round(100 * (1 - var(d2$SOC - p) / var(d2$SOC)), 1)
```


## Cross validation

Instead of the data splitting that we used above, you always want to use the *full dataset* to fit your final model. The model that you will use. To evaluate the model, you should cross-validation. 

In cross-validation the data is divided into *k* groups. Typically into 5 or 10 groups. Each group is used once for model testing, and *k-1* times for model training. An extreme case that is "leave-one out", where *k* is equal to the number of records, bu this is generally not considered a good practise.

Let's make the groups. 5 in this case. 
```{r}
n <- 5
library(dismo)
set.seed(31415)
k <- kfold(d, n)
table(k)
```

Now do the cross-validation, and compute a number of statistics of interest. 

```{r}
rfRMSE <- rfRMSEnull <- rfvarexp <- rfcor <- rep(NA, n)
for (i in 1:n) {
	test <- d[k==i, ]
	train <- d[k!=i, ]
	m <- randomForest(model, data=train)
	p <- predict(m, test)
	rfRMSE[i] <- rmse(test$SOC, p)
	rfRMSEnull[i] <- rmsenull(test$SOC, p)
	rfvarexp[i] <- var(test$SOC - p) / var(test$SOC)
	rfcor[i] <- cor(test$SOC, p)
}
mean(rfRMSE)
mean(rfRMSEnull)
mean(rfvarexp)
mean(rfcor)
```

We can use the same procedure for any other predictive model. Here we show that for our linear regression model.

```{r}
lmRMSE <- lmRMSEnull <- lmvarexp <- lmcor <- rep(NA, n)
for (i in 1:n) {
	test <- d[k==i, ]
	train <- d[k!=i, ]
	m <- lm(model, data=train)
	p <- predict(m, test)
	lmRMSE[i] <- rmse(test$SOC, p)
	lmRMSEnull[i] <- rmsenull(test$SOC, p)
	lmvarexp[i] <- var(test$SOC - p) / var(test$SOC)
	lmcor[i] <- cor(test$SOC, p)
}
mean(lmRMSE)
mean(lmRMSEnull)
mean(lmvarexp)
mean(lmcor)
```

An important purpose of cross-validation is to get a sense of the quality of the model. But this is often not straighforward. Whether the quality of a model is sufficient depends on the purpose, and may require further numerical analysis. 

Using cross-validation results is more straighforward in the context of model comparison. It can be used to select the best model, or, often more appropriate, to average a set of good models --- perhaps a average weighted by the RMSE. 

Finally, cross-valdiation is important to find optimal values for "nuisance parameters" that needt to be set to regularize or otherwise parametrize a model. Examples for the randomForest methods are parameters such as `ntry` and `nodesize`. See ?randomForest.


## Opening the box

Machine learning type regression models are sometimes described as "black boxes" --- we cannot see what is going on inside. Afer all, we do not have a few simple parameters as we might have with a linear regression model. Well, the box has a lid, and we can look inside. 

Here we show two general methods, "variable importance" and "partial response" that are available in for the R randomForest type models, but are really applicable to any predictive model.


### Variable importance

Which variables are important, which are not?

```{r}
rfm <- randomForest(model, data=d)
varImpPlot(rfm)
```

Intuitively this is very easy to understand: "clay" and "precip" are very important; "temp" is not. The "Increase in Node Purity" (IncNodePurity) expresses the change in the homogeneity of the of the groups created by the trees (using the Gini coefficient as a measure). What is expressed is the decrease in said purity if a particular variable has no information. If a variable has no information to begin with, the decrase would be zero. 

The notion of node purity is specific to tree-models. But the notion of variable importance is not. We can also use the change in RMSE to look at variable importance. 

Here is a function that computes variable importance for any model (that has a "predict" methods) in R (but only for quantitative variables; it is little more work for categorical variables).

To assess importance for a variable, we randomize its values of that variable, without touching the other variables. We then use the model to make a prediction and compute a model evaluation statistic (here we use RMSE). Because of the vagaries of randomization we do this a number of times. We compare the average RMSE of the model predictions made with the randomized data, with the RMSE of predictions with the original data. If the difference is large, the variable is important. If the difference is small, the variable is not important.  

```{r}
varimp <- function(mod, dat, vars, n=10) {
	RMSE <- matrix(nrow=n, ncol=length(vars))
	colnames(RMSE) <- vars
	for (i in 1:length(vars)) {
	rd <- dat
	v <- vars[i]
		for (j in 1:n) {
			rd[[v]] <- sample(rd[[v]])
			p <- predict(mod, rd)
			RMSE[j,i] <- rmse(rd$SOC, p)
		}
	}
	return(RMSE)
}
``` 

Let's use the fuction for our Random Forest model.

```{r, pvars}
predvars <- c("pH", "precip", "clay", "sand", "temp")
vi <- varimp(rfm, d, predvars)
vimean <- colMeans(vi)
p <- predict(m, d)
RMSEfull <- rmse(d$SOC, p)
x <- sort(vimean - RMSEfull)
dotchart(x)
```

Not exactly the same as what `varImpPlot` gave us; but pretty much the same message.

We can use the same function for the linear regression model

```{r}
mlr <- lm(model, data=d)
vi <- varimp(mlr, d, predvars)
vimean <- colMeans(vi)
p <- predict(m, d)
RMSEfull <- rmse(d$SOC, p)
x <- sort(vimean - RMSEfull)
dotchart(x)
```

### Partial response plots 

Another interesting concept is the partial response plot. It shows the response of the model to one variable, with the other variables held constant.

```{r}
par(mfrow=c(2,2), mai=c(.75,rep(0.5,3)))
partialPlot(rfm, d, "pH")
partialPlot(rfm, d, "clay")
partialPlot(rfm, d, "sand")
partialPlot(rfm, d, "precip")
```

Do you think these responses make sense? That is, do they conform to what you know about soil science (if anything)?

You have to interpret these plots with caution as it does not show interactions; and these can be very important. 

To show interactions we can make partial plots for two variables (to be done), but let's first make a general implementation. The `partialPlot` function comes with the `randomForest` package. Here is a generic implementation that works with any model with a predict method.

In the function we create a sequence of values for the variable of interest. We then loop over that sequence. In each iteration, all values for the variable of interest in the original data are replaced with a single value from the sequence. The values of all other variables stay the same. The model is used to make a prediction for all records, and these predictions are averaged. Together these predictions form the partial response.

```{r}
partialresponse <- function(mod, dat, var, nsteps=25, rng=NULL) {
	if (is.null(rng)) { rng <- range(dat[[var]]) }
	increment <- (rng[2] - rng[1])/(nsteps-2)
	steps <- seq(rng[1]-increment, rng[2]+increment, increment)
	res <- rep(NA, length(steps))
	for (i in 1:length(steps)) {
		dat[[var]] <- steps[i]
		p <- predict(mod, dat)
		res[i] <- mean(p)
	}
	res <- cbind(steps, res)
	colnames(res) <- c(var, "p")
	res
}
``` 

Let's use it for `pH` with the Random Forest model


```{r}
pr_pH <- partialresponse(rfm, d, "pH")
plot(pr_pH, type="l")
rug(quantile(d$pH, seq(0, 1, 0.1)))
```

Very similar to what the `partialPlot` function returned. 

And now for the linear regression model. 

```{r}
lrm <- lm(model, data=d)
pr_pH <- partialresponse(lrm, d, "pH")
plot(pr_pH, type="l")
rug(quantile(d$pH, seq(0, 1, 0.1)))
```

OK, that one is not too surprising. But it is nice that it works for *any* regression type model.


## Conclusions

- statistical modeling for inference is not the same as prediction

- a major concern in prediction is the bias-variance trade-off (underfitting, overfitting)

- predictive models are evaluated with cross-validation 

- cross-validation is also used to estimate ("nuisance") parameters 

- there are general tools to inspect the properties of predictive models (variable importance, partial responses).

- machine learning is easy to do, but harder to understand, at first

- machine learning algorithms are not that hard to understand!


### What we did not discuss

- Random Forest can also do supervised classification (that is, predict classes) --- see an example [here](/tools/remote-sensing/crops.html).

- Other examples:  

- Random Forest can also do unsupervised classification... 



## Citation

Hijmans, R.J., 2019. Statistical modeling. In: Hijmans, R.J. and J. Chamberlin. Regional Agronomy: a pratical handbook. CIMMYT. https:/reagro.org/tools/statistical.html


